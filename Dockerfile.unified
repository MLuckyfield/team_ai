FROM node:18-slim

# Install system dependencies for all services
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    git \
    build-essential \
    supervisor \
    nginx \
    # Python dependencies for Crawl4AI
    python3 \
    python3-pip \
    python3-dev \
    # Playwright dependencies for Crawl4AI
    libnss3 \
    libnspr4 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libcups2 \
    libdrm2 \
    libdbus-1-3 \
    libxcb1 \
    libxkbcommon0 \
    libx11-6 \
    libxcomposite1 \
    libxdamage1 \
    libxext6 \
    libxfixes3 \
    libxrandr2 \
    libgbm1 \
    libpango-1.0-0 \
    libcairo2 \
    libasound2 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy source code
COPY crawl4ai-source ./crawl4ai-source
COPY server.js ./
COPY package.json ./
COPY index.html ./

# Install n8n globally
RUN npm install -g n8n@1.70.1

# Install landing page dependencies
RUN npm install

# Install Crawl4AI and its dependencies
WORKDIR /app/crawl4ai-source
RUN pip3 install --upgrade pip --break-system-packages
RUN pip3 install -r requirements.txt --break-system-packages
# Install the missing dns dependency
RUN pip3 install dnspython --break-system-packages
RUN pip3 install -e . --break-system-packages

# Install playwright browsers for Crawl4AI
RUN playwright install chromium
RUN playwright install-deps

# Install Crawl4AI server dependencies
WORKDIR /app/crawl4ai-source/deploy/docker
RUN pip3 install uvicorn fastapi slowapi pydantic --break-system-packages

# Go back to app directory
WORKDIR /app

# Copy config files
COPY nginx.conf /etc/nginx/nginx.conf
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# Expose ports
EXPOSE 80 5678 11235 8080

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV N8N_HOST=0.0.0.0
ENV N8N_PORT=5678
ENV NODE_ENV=production
ENV DISPLAY=:99

# Start supervisor to manage all services
CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"] 